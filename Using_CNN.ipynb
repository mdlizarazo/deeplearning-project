{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53TEqhh7ZWVL"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torchaudio\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Directory path to your dataset\n",
        "dataset_path = '/content/drive/MyDrive/AudioWAV/'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create empty lists for emotions and file paths\n",
        "file_emotion = []\n",
        "file_paths = []\n",
        "\n",
        "# Iterate through all files in the directory\n",
        "for file_name in os.listdir(dataset_path):\n",
        "    # Construct the full file path\n",
        "    file_path_full = os.path.join(dataset_path, file_name)\n",
        "\n",
        "    # Check if the file is an audio file (assuming WAV format)\n",
        "    if file_name.lower().endswith('.wav') and os.path.isfile(file_path_full):\n",
        "        # storing file paths\n",
        "        file_paths.append(file_path_full)\n",
        "\n",
        "        # storing file emotions\n",
        "        part = file_name.split('_')\n",
        "        if part[2] == 'SAD':\n",
        "            file_emotion.append('sad')\n",
        "        elif part[2] == 'ANG':\n",
        "            file_emotion.append('angry')\n",
        "        elif part[2] == 'DIS':\n",
        "            file_emotion.append('disgust')\n",
        "        elif part[2] == 'FEA':\n",
        "            file_emotion.append('fear')\n",
        "        elif part[2] == 'HAP':\n",
        "            file_emotion.append('happy')\n",
        "        elif part[2] == 'NEU':\n",
        "            file_emotion.append('neutral')\n",
        "        else:\n",
        "            file_emotion.append('Unknown')\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(file_emotion)"
      ],
      "metadata": {
        "id": "ZptvY_maZXzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_type_s = 'kaiser_best'\n",
        "duration_s = None\n",
        "sample_rate_s = 22050\n",
        "offset_s = 0.5"
      ],
      "metadata": {
        "id": "L5oyuYaIZZZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "librosa_audio, librosa_sample_rate = librosa.load(file_path_full)\n",
        "\n",
        "mfccs = librosa.feature.mfcc(y=librosa_audio, sr=librosa_sample_rate, n_mfcc=40)\n",
        "print(mfccs.shape)\n",
        "\n",
        "import librosa.display\n",
        "librosa.display.specshow(mfccs, sr=librosa_sample_rate, x_axis='time')"
      ],
      "metadata": {
        "id": "9mG5OCJ0Za_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Get audio features\n",
        "\n",
        "audio_inputs = [librosa.load(file_path_full, sr=16000) for file_path_full in file_paths]\n",
        "\n",
        "#Get MFFC features\n",
        "#mfccs = librosa.feature.mfcc(y=X)\n",
        "#Get MFFCs average features\n",
        "#mfccs_mean = np.mean(    mfccs,\n",
        "#                          axis = 0) ################check axis 0/1"
      ],
      "metadata": {
        "id": "glgLcDMoZcf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X =[]\n",
        "\n",
        "for audio in audio_inputs:\n",
        "  array, sr = audio\n",
        "  X.append(array)\n",
        "\n",
        "\n",
        "print(X[0])"
      ],
      "metadata": {
        "id": "U5z_UISdZemF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert audio files to mel spectrograms using librosa\n",
        "#spectrograms = [librosa.feature.melspectrogram(y=torchaudio.load(file_path_full)[0].numpy().flatten(), sr=16000) for file_path_full in file_paths]\n",
        "#audio_inputs = [librosa.load(file_path_full, sr=16000) for file_path_full in file_paths]\n",
        "\n",
        "# Data splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "\n",
        "print(len(X_train))"
      ],
      "metadata": {
        "id": "m3CP8Q10ZgFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, Flatten, Dropout\n",
        "\n",
        "#Build sequential CNN\n",
        "CNN_model = Sequential()\n",
        "\n",
        "#Build first layer\n",
        "CNN_model.add(Conv1D(16, 5,padding='same',input_shape=(5357, 80080), activation='relu'))\n",
        "\n",
        "#Build second layer\n",
        "CNN_model.add(Conv1D(32, 5,padding='same',activation='relu'))\n",
        "\n",
        "#Build third layer\n",
        "CNN_model.add(Conv1D(64, 5,padding='same',activation='relu'))\n",
        "\n",
        "#Build forth layer\n",
        "CNN_model.add(Conv1D(128, 5,padding='same',activation='relu'))\n",
        "\n",
        "#Add dropout\n",
        "CNN_model.add(Dropout(0.1))\n",
        "\n",
        "#Flatten\n",
        "CNN_model.add(Flatten())\n",
        "\n",
        "CNN_model.add(Dense(128, activation ='relu'))\n",
        "CNN_model.add(Dropout(0.1))\n",
        "CNN_model.add(Dense(64, activation ='relu'))\n",
        "CNN_model.add(Dense(8, activation='softmax'))"
      ],
      "metadata": {
        "id": "TVUyQon7Zhn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model with the desired loss function, optimizer, and metric to optimize\n",
        "CNN_model.compile(loss = 'categorical_crossentropy',\n",
        "                  optimizer = 'Adam',\n",
        "                  metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "CRRaWyGcZkSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.array(X_train).shape)"
      ],
      "metadata": {
        "id": "hMpxiTiPZm-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "X_train = pad_sequences(X_train)\n",
        "print(X_train.shape)\n"
      ],
      "metadata": {
        "id": "Hv2z57eNZtFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "7rdeZsQgZupr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model fit\n",
        "cnn_results = CNN_model.fit(X_train, y_train,\n",
        "              batch_size = 64,\n",
        "              epochs = 25,\n",
        "              verbose = 1,\n",
        "              validation_data = (X_val, y_val))"
      ],
      "metadata": {
        "id": "n_AMKBp5ZwLE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}